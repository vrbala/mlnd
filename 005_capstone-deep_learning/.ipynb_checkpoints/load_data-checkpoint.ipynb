{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Here we attempt to load, explore and clean if required the data from street view housing numbers dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from scipy import io\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: data.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified data.tar.gz\n"
     ]
    }
   ],
   "source": [
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 1% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, url, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url, filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('data.tar.gz', 'https://www.dropbox.com/s/yiogni0r2ac2aym/data.tar.gz?dl=1', 184858650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    tar = tarfile.open('data.tar.gz')\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the digitStruct.csv file and augment it with useful information\n",
    "def read_csv(filename):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    df['LabelCount'] = df.DigitLabel\n",
    "    df.DigitLabel = df['DigitLabel'].replace(10, 0)\n",
    "    # right and bottom most offsets\n",
    "    df['RightMost'] = df.Left + df.Width\n",
    "    df['BottomMost'] = df.Top + df.Height\n",
    "\n",
    "    grouped = df.groupby(['FileName'])\n",
    "    agg = grouped.agg({'Left': np.min, 'Top': np.min, 'BottomMost': np.max, 'RightMost': np.max, 'LabelCount': np.size, 'DigitLabel': lambda x: tuple(x)})\n",
    "    agg['Height'] = agg.BottomMost - agg.Top\n",
    "    agg['Width'] = agg.RightMost - agg.Left\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [],
   "source": [
    "image_size = 48  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, csvData):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size, 3),\n",
    "                         dtype=np.float32)\n",
    "  # length and 5 digits - 10 to signify absence of a digit in sequence\n",
    "  labels = np.ndarray(shape=(len(image_files), 6), dtype=np.int8)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      # first - label data\n",
    "      label_data = csvData.loc[image]\n",
    "      numbers = list(label_data.DigitLabel)\n",
    "      # if length of the numbers is less than 5, pad it with 10s\n",
    "      if len(numbers) < 5:\n",
    "        numbers += [10]*(5-len(numbers))\n",
    "      elif len(numbers) > 5:\n",
    "        continue # skip this image. number of digits more than 5\n",
    "      \n",
    "      length = label_data.LabelCount-1 # lengths as labels from 0 to 4 where they are actually (1 to 5)\n",
    "      labels[num_images, :] = np.array([length] + numbers)\n",
    "    \n",
    "      # now image data\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size, 3):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :, :] = image_data        \n",
    "    \n",
    "      # Increment the index counter\n",
    "      num_images = num_images + 1\n",
    "      \n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :, :]\n",
    "  labels = labels[0:num_images, :]\n",
    "  \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Full labels tensor:', labels.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return (dataset, labels)\n",
    "\n",
    "train_dataset, train_labels = load_letter('data/train', read_csv('data/train_digitStruct.csv'))\n",
    "test_dataset, test_labels = load_letter('data/test', read_csv('data/test_digitStruct.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_labels.shape[0])\n",
    "for i in permutation[:10]:\n",
    "    print(train_labels[i])\n",
    "    plt.figure()\n",
    "    plt.imshow(train_dataset[i])\n",
    "    del i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "kf = ShuffleSplit(train_labels.shape[0], random_state=0, train_size=0.9)\n",
    "train_idx, valid_idx = next(iter(kf))\n",
    "train_dataset_new = train_dataset[train_idx]\n",
    "train_labels_new = train_labels[train_idx]\n",
    "valid_dataset = train_dataset[valid_idx]\n",
    "valid_labels = train_labels[valid_idx]\n",
    "train_dataset = train_dataset_new\n",
    "train_labels = train_labels_new\n",
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_dataset.shape)\n",
    "print(test_labels.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'SVHN_multi_48.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
